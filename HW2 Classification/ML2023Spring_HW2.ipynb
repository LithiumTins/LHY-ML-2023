{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2: Phoneme Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7DRC5V7_8A5"
      },
      "source": [
        "Objectives:\n",
        "* Solve a classification problem with deep neural networks (DNNs).\n",
        "* Understand recursive neural networks (RNNs).\n",
        "\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "# Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have\n",
        "- `libriphone/train_split.txt`: training metadata\n",
        "- `libriphone/train_labels`: training labels\n",
        "- `libriphone/test_split.txt`: testing metadata\n",
        "- `libriphone/feat/train/*.pt`: training feature\n",
        "- `libriphone/feat/test/*.pt`:  testing feature\n",
        "\n",
        "after running the following block.\n",
        "\n",
        "> **Notes: if the google drive link is dead, you can download the data directly from [Kaggle](https://www.kaggle.com/c/ml2023spring-hw2/data) and upload it to the workspace.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzkiMEcC3Foq"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade gdown\n",
        "\n",
        "# # Main link\n",
        "# !gdown --id '1N1eVIDe9hKM5uiNRGmifBlwSDGiVXPJe' --output libriphone.zip\n",
        "# # !gdown --id '1qzCRnywKh30mTbWUEjXuNT2isOCAPdO1' --output libriphone.zip\n",
        "\n",
        "# !unzip -q libriphone.zip\n",
        "# !ls libriphone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pADUiYODJE1O"
      },
      "source": [
        "# Some Utility Functions\n",
        "**Fixes random number generator seeds for reproducibility.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BsZKgBZQJjaE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def same_seeds(seed):\n",
        "    random.seed(seed) \n",
        "    np.random.seed(seed)  \n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "**Helper functions to pre-process the training data from raw MFCC features of each utterance.**\n",
        "\n",
        "A phoneme may span several frames and is dependent to past and future frames. \\\n",
        "Hence we concatenate neighboring phonemes for training to achieve higher accuracy. The **concat_feat** function concatenates past and future k frames (total 2k+1 = n frames), and we predict the center frame.\n",
        "\n",
        "Feel free to modify the data preprocess functions, but **do not drop any frame** (if you modify the functions, remember to check that the number of frames are the same as mentioned in the slides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJjLT8em-y9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_feat(path):\n",
        "    feat = torch.load(path)\n",
        "    return feat\n",
        "\n",
        "def shift(x, n):\n",
        "    if n < 0:\n",
        "        left = x[0].repeat(-n, 1)\n",
        "        right = x[:n]\n",
        "    elif n > 0:\n",
        "        right = x[-1].repeat(n, 1)\n",
        "        left = x[n:]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "    return torch.cat((left, right), dim=0)\n",
        "\n",
        "def concat_feat(x, concat_n):\n",
        "    assert concat_n % 2 == 1 # n must be odd\n",
        "    if concat_n < 2:\n",
        "        return x\n",
        "    seq_len, feature_dim = x.size(0), x.size(1)\n",
        "    x = x.repeat(1, concat_n) \n",
        "    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n",
        "    mid = (concat_n // 2)\n",
        "    for r_idx in range(1, mid+1):\n",
        "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
        "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
        "\n",
        "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
        "\n",
        "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8):\n",
        "    class_num = 41 # NOTE: pre-computed, should not need change\n",
        "\n",
        "    if split == 'train' or split == 'val':\n",
        "        mode = 'train'\n",
        "    elif split == 'test':\n",
        "        mode = 'test'\n",
        "    else:\n",
        "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
        "\n",
        "    label_dict = {}\n",
        "    if mode == 'train':\n",
        "        for line in open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines():\n",
        "            line = line.strip('\\n').split(' ')\n",
        "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
        "        \n",
        "        # split training and validation data\n",
        "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
        "        random.shuffle(usage_list)\n",
        "        train_len = int(len(usage_list) * train_ratio)\n",
        "        train_list = usage_list[:train_len]\n",
        "        val_list = usage_list[train_len:]\n",
        "\n",
        "    elif mode == 'test':\n",
        "        test_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
        "\n",
        "    # 略微魔改了一下关于数据划分的代码，避免出现训练验证集交叉的可能\n",
        "    def transform_data(usage_list, split):\n",
        "        usage_list = [line.strip('\\n') for line in usage_list]\n",
        "        print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
        "\n",
        "        max_len = 3000000\n",
        "        X = torch.empty(max_len, 39 * concat_nframes)\n",
        "        if mode == 'train':\n",
        "            y = torch.empty(max_len, dtype=torch.long)\n",
        "\n",
        "        idx = 0\n",
        "        for i, fname in tqdm(enumerate(usage_list)):\n",
        "            feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
        "            cur_len = len(feat)\n",
        "            feat = concat_feat(feat, concat_nframes)\n",
        "            if mode == 'train':\n",
        "                label = torch.LongTensor(label_dict[fname])\n",
        "\n",
        "            X[idx: idx + cur_len, :] = feat\n",
        "            if mode == 'train':\n",
        "                y[idx: idx + cur_len] = label\n",
        "\n",
        "            idx += cur_len\n",
        "\n",
        "        X = X[:idx, :]\n",
        "        if mode == 'train':\n",
        "            y = y[:idx]\n",
        "\n",
        "        print(f'[INFO] {split} set')\n",
        "        print(X.shape)\n",
        "        if mode == 'train':\n",
        "            print(y.shape)\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "    \n",
        "    if mode == 'test':\n",
        "        return transform_data(test_list, \"test\")\n",
        "    \n",
        "    return *transform_data(train_list, \"train\"), *transform_data(val_list, \"val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LibriDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = X\n",
        "        if y is not None:\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "# Model\n",
        "Feel free to modify the structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-GRd7ywdrL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout=0.5):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # TODO: apply batch normalization and dropout for strong baseline.\n",
        "        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html (batch normalization)\n",
        "        #       https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html (dropout)\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "\n",
        "# 这是 strong baseline 的模型\n",
        "# class Classifier(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256):\n",
        "#         super(Classifier, self).__init__()\n",
        "\n",
        "#         self.fc = nn.Sequential(\n",
        "#             BasicBlock(input_dim, hidden_dim, 0.3),\n",
        "#             *[BasicBlock(hidden_dim, hidden_dim, 0.5) for _ in range(hidden_layers)],\n",
        "#             nn.Linear(hidden_dim, output_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# 这是 boss baseline 的模型\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.input_size = 39\n",
        "        self.hidden_size = 512\n",
        "        self.num_layers = 5\n",
        "        self.linear_hidden_dim = 256\n",
        "        self.output_dim = 41\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=0.5, bidirectional=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2 * self.hidden_size, self.linear_hidden_dim),\n",
        "            nn.BatchNorm1d(self.linear_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.linear_hidden_dim, self.output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1, self.input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, x.shape[1] // 2]\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlIq8JeqvvHC"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIHn79Iav1ri"
      },
      "outputs": [],
      "source": [
        "# data prarameters\n",
        "# TODO: change the value of \"concat_nframes\" for medium baseline\n",
        "concat_nframes = 77   # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
        "# strong baseline 使用 concat 23\n",
        "train_ratio = 0.95   # the ratio of data used for training, the rest will be used for validation\n",
        "\n",
        "# training parameters\n",
        "seed = 1213          # random seed\n",
        "batch_size = 512        # batch size\n",
        "num_epoch = 50         # the number of training epoch\n",
        "learning_rate = 1e-4      # learning rate\n",
        "model_path = './model.ckpt'  # the path where the checkpoint will be saved\n",
        "\n",
        "# model parameters\n",
        "# TODO: change the value of \"hidden_layers\" or \"hidden_dim\" for medium baseline\n",
        "input_dim = 39 * concat_nframes  # the input dim of the model, you should not change the value\n",
        "hidden_layers = 3          # the number of hidden layers\n",
        "hidden_dim = 1024           # the hidden dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIUFRgG5yoDn"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c1zI3v5jyrDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n",
            "[Dataset] - # phone classes: 41, number of utterances for train: 3257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/tmp/ipykernel_4044762/3769724421.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  feat = torch.load(path)\n",
            "3257it [00:58, 55.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] train set\n",
            "torch.Size([2007632, 3003])\n",
            "torch.Size([2007632])\n",
            "[Dataset] - # phone classes: 41, number of utterances for val: 172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "172it [00:03, 56.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] val set\n",
            "torch.Size([109162, 3003])\n",
            "torch.Size([109162])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "\n",
        "same_seeds(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# preprocess data\n",
        "train_X, train_y, val_X, val_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio)\n",
        "\n",
        "# get dataset\n",
        "train_set = LibriDataset(train_X, train_y)\n",
        "val_set = LibriDataset(val_X, val_y)\n",
        "\n",
        "# remove raw feature to save memory\n",
        "del train_X, train_y, val_X, val_y\n",
        "gc.collect()\n",
        "\n",
        "# get dataloader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWH1KIqzxEr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdMWsBs7zzNs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/050] Train Acc: 0.70164 Loss: 1.05290 | Val Acc: 0.78419 loss: 0.72085\n",
            "saving model with acc 0.78419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[002/050] Train Acc: 0.84006 Loss: 0.53692 | Val Acc: 0.80672 loss: 0.68774\n",
            "saving model with acc 0.80672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[003/050] Train Acc: 0.88232 Loss: 0.37664 | Val Acc: 0.81097 loss: 0.74185\n",
            "saving model with acc 0.81097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[004/050] Train Acc: 0.90776 Loss: 0.27719 | Val Acc: 0.81458 loss: 0.79339\n",
            "saving model with acc 0.81458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[005/050] Train Acc: 0.92228 Loss: 0.22386 | Val Acc: 0.81679 loss: 0.83186\n",
            "saving model with acc 0.81679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[006/050] Train Acc: 0.93150 Loss: 0.19103 | Val Acc: 0.81960 loss: 0.85681\n",
            "saving model with acc 0.81960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.88it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[007/050] Train Acc: 0.93787 Loss: 0.16855 | Val Acc: 0.81956 loss: 0.91104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:06<00:00,  5.88it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[008/050] Train Acc: 0.94338 Loss: 0.15116 | Val Acc: 0.82255 loss: 0.92399\n",
            "saving model with acc 0.82255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[009/050] Train Acc: 0.94736 Loss: 0.13867 | Val Acc: 0.81684 loss: 1.00214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[010/050] Train Acc: 0.95059 Loss: 0.12852 | Val Acc: 0.82445 loss: 0.97312\n",
            "saving model with acc 0.82445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[011/050] Train Acc: 0.95347 Loss: 0.11980 | Val Acc: 0.82277 loss: 1.02059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.87it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[012/050] Train Acc: 0.95607 Loss: 0.11244 | Val Acc: 0.82442 loss: 1.05078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[013/050] Train Acc: 0.95815 Loss: 0.10650 | Val Acc: 0.82084 loss: 1.07754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[014/050] Train Acc: 0.96000 Loss: 0.10129 | Val Acc: 0.82409 loss: 1.10502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[015/050] Train Acc: 0.96210 Loss: 0.09606 | Val Acc: 0.82095 loss: 1.11914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 18.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[016/050] Train Acc: 0.96364 Loss: 0.09186 | Val Acc: 0.82119 loss: 1.13469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[017/050] Train Acc: 0.96545 Loss: 0.08745 | Val Acc: 0.82583 loss: 1.12631\n",
            "saving model with acc 0.82583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[018/050] Train Acc: 0.96677 Loss: 0.08400 | Val Acc: 0.82209 loss: 1.18958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[019/050] Train Acc: 0.96796 Loss: 0.08128 | Val Acc: 0.82436 loss: 1.14719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[020/050] Train Acc: 0.96940 Loss: 0.07767 | Val Acc: 0.82398 loss: 1.15922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:08<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 18.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[021/050] Train Acc: 0.97065 Loss: 0.07460 | Val Acc: 0.82606 loss: 1.17669\n",
            "saving model with acc 0.82606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:09<00:00,  5.86it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[022/050] Train Acc: 0.97160 Loss: 0.07233 | Val Acc: 0.82411 loss: 1.21535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:07<00:00,  5.88it/s]\n",
            "100%|██████████| 214/214 [00:11<00:00, 19.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[023/050] Train Acc: 0.97288 Loss: 0.06956 | Val Acc: 0.82814 loss: 1.22515\n",
            "saving model with acc 0.82814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:05<00:00,  5.90it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[024/050] Train Acc: 0.97366 Loss: 0.06755 | Val Acc: 0.82525 loss: 1.24748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:04<00:00,  5.90it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[025/050] Train Acc: 0.97471 Loss: 0.06494 | Val Acc: 0.82465 loss: 1.27961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:04<00:00,  5.91it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[026/050] Train Acc: 0.97540 Loss: 0.06303 | Val Acc: 0.82557 loss: 1.26197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:04<00:00,  5.90it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[027/050] Train Acc: 0.97636 Loss: 0.06110 | Val Acc: 0.82606 loss: 1.27259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:04<00:00,  5.90it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[028/050] Train Acc: 0.97708 Loss: 0.05916 | Val Acc: 0.82662 loss: 1.29241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:05<00:00,  5.90it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[029/050] Train Acc: 0.97784 Loss: 0.05753 | Val Acc: 0.82820 loss: 1.29363\n",
            "saving model with acc 0.82820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [11:05<00:00,  5.89it/s]\n",
            "100%|██████████| 214/214 [00:10<00:00, 19.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[030/050] Train Acc: 0.97847 Loss: 0.05615 | Val Acc: 0.82604 loss: 1.31722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 662/3922 [01:52<09:14,  5.88it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     optimizer.step() \n\u001b[32m     28\u001b[39m     _, train_pred = torch.max(outputs, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# get the index of the class with the highest probability\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     train_acc += \u001b[43m(\u001b[49m\u001b[43mtrain_pred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     train_loss += loss.item()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# validation\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# create model, define a loss function, and optimizer\n",
        "\n",
        "# strong baseline\n",
        "# model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "\n",
        "# boss baseline\n",
        "model = Classifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        features, labels = batch\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(features) \n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "        \n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # validation\n",
        "    model.eval() # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(val_loader)):\n",
        "            features, labels = batch\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(features)\n",
        "            \n",
        "            loss = criterion(outputs, labels) \n",
        "            \n",
        "            _, val_pred = torch.max(outputs, 1) \n",
        "            val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc/len(train_set):3.5f} Loss: {train_loss/len(train_loader):3.5f} | Val Acc: {val_acc/len(val_set):3.5f} loss: {val_loss/len(val_loader):3.5f}')\n",
        "\n",
        "    # if the model improves, save a checkpoint at this epoch\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f'saving model with acc {best_acc/len(val_set):.5f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ab33MxosWLmG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del train_set, val_set\n",
        "del train_loader, val_loader\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "# Testing\n",
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VOG1Ou0PGrhc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dataset] - # phone classes: 41, number of utterances for test: 857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/tmp/ipykernel_4044762/3769724421.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  feat = torch.load(path)\n",
            "857it [00:09, 86.93it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] test set\n",
            "torch.Size([527364, 3003])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes)\n",
        "test_set = LibriDataset(test_X, None)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay0Fu8Ovkdad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4044762/2059846564.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load model\n",
        "\n",
        "# strong baseline\n",
        "# model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "\n",
        "# boss baseline\n",
        "model = Classifier().to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp-DV1p4r7Nz"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1031/1031 [00:51<00:00, 20.04it/s]\n"
          ]
        }
      ],
      "source": [
        "pred = np.array([], dtype=np.int32)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader)):\n",
        "        features = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        outputs = model(features)\n",
        "\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZqy40Prz0v"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(pred):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
